{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb6d6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded .env: c:\\Kaggle-agent\\.env\n",
      "âœ… Setup and authentication complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from dotenv import find_dotenv, load_dotenv  \n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\n",
    "        \"python-dotenv is not installed. Install it with: pip install python-dotenv\"\n",
    "    ) from e\n",
    "\n",
    "env_path = find_dotenv(filename=\".env\", usecwd=True)\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"âœ… Loaded .env: {env_path}\")\n",
    "else:\n",
    "    # Still attempt default behavior (may load from other locations)\n",
    "    load_dotenv()\n",
    "    print(f\"No .env found via find_dotenv(). CWD: {Path.cwd()}\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"Missing GOOGLE_API_KEY. Ensure your .env is in the current working directory \"\n",
    "        f\"({Path.cwd()}) and contains GOOGLE_API_KEY=... then restart the kernel and re-run this cell.\"\n",
    "    )\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "if not TAVILY_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"Missing TAVILY_API_KEY. Ensure your .env is in the current working directory \"\n",
    "        f\"({Path.cwd()}) and contains TAVILY_API_KEY=... then restart the kernel and re-run this cell.\"\n",
    "    )\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"Missing GROQ_API_KEY. Ensure your .env is in the current working directory \"\n",
    "        f\"({Path.cwd()}) and contains GROQ_API_KEY=... then restart the kernel and re-run this cell.\"\n",
    "    )\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "print(\"âœ… Setup and authentication complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e287e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ADK components imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "from google.adk.agents import Agent, LlmAgent\n",
    "from google.adk.apps.app import App, EventsCompactionConfig\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.sessions import DatabaseSessionService\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.genai import types\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.tools import google_search\n",
    "from google.adk.agents import ParallelAgent, SequentialAgent\n",
    "from google.adk.memory import InMemoryMemoryService\n",
    "from google.adk.tools import load_memory, preload_memory\n",
    "from google.adk.agents.remote_a2a_agent import (\n",
    "    RemoteA2aAgent,\n",
    "    AGENT_CARD_WELL_KNOWN_PATH,\n",
    ")\n",
    "from google.adk.a2a.utils.agent_to_a2a import to_a2a\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\" ADK components imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe792d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# HELPER FUNCTION\n",
    "\n",
    "async def run_session(runner_instance: Runner, user_queries: list[str] | str, session_id: str = \"default\"):\n",
    "    \"\"\"Helper function to run queries in a session and display responses.\"\"\"\n",
    "    print(f\"\\n### Session: {session_id}\")\n",
    "\n",
    "    # Create or retrieve session\n",
    "    try:\n",
    "        session = await session_service.create_session(\n",
    "            app_name=APP_NAME, user_id=USER_ID, session_id=session_id\n",
    "        )\n",
    "    except:\n",
    "        session = await session_service.get_session(\n",
    "            app_name=APP_NAME, user_id=USER_ID, session_id=session_id\n",
    "        )\n",
    "\n",
    "    # Convert single query to list\n",
    "    if isinstance(user_queries, str):\n",
    "        user_queries = [user_queries]\n",
    "\n",
    "    # Process each query\n",
    "    for query in user_queries:\n",
    "        print(f\"\\nUser > {query}\")\n",
    "        query_content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "\n",
    "        # Stream agent response\n",
    "        async for event in runner_instance.run_async(\n",
    "            user_id=USER_ID, session_id=session.id, new_message=query_content\n",
    "        ):\n",
    "            if event.is_final_response() and event.content and event.content.parts:\n",
    "                text = event.content.parts[0].text\n",
    "                if text and text != \"None\":\n",
    "                    print(f\"Model: > {text}\")\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3885ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_config=types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1, # Initial delay before first retry (in seconds)\n",
    "    http_status_codes=[429, 500, 503, 504] # Retry on these HTTP errors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c63886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Services initialized.\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from google.adk.agents import LlmAgent\n",
    "\n",
    "litellm.set_verbose = False\n",
    "litellm.suppress_debug_info = True\n",
    "\n",
    "# Setup Services and model\n",
    "memory_service = InMemoryMemoryService()\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# llm_model = Gemini(model_id=\"gemini-2.0-flash-exp\", retry_options=retry_config)\n",
    "\n",
    "# llm_model = LiteLlm(\n",
    "#     model=\"openrouter/meta-llama/llama-3.3-70b-instruct:free\",\n",
    "#     api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "#     api_base=\"https://openrouter.ai/api/v1\",\n",
    "# )\n",
    "\n",
    "# llm_model = LiteLlm(\n",
    "#     model=\"ollama_chat/qwen2.5:1.5b\",\n",
    "#     api_base=\"http://localhost:11434\",\n",
    "# )\n",
    "\n",
    "llm_model = LiteLlm(\n",
    "    model=\"openai/llama-3.1-8b-instant\", \n",
    "    # model=\"groq/compound\",\n",
    "    # model=\"openai/gpt-oss-120b\",\n",
    "    # model=\"openai/meta-llama/llama-4-scout-17b-16e-instruct\", \n",
    "    api_key=GROQ_API_KEY,\n",
    "    api_base=\"https://api.groq.com/openai/v1\" \n",
    ")\n",
    "\n",
    "# Helper to auto-save memory after every turn \n",
    "async def auto_save_to_memory(callback_context):\n",
    "    await callback_context._invocation_context.memory_service.add_session_to_memory(\n",
    "        callback_context._invocation_context.session\n",
    "    )\n",
    "\n",
    "print(\" Services initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d45348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# try:\n",
    "#     print(\"ðŸ“¡ Testing connection to local Ollama...\")\n",
    "#     # Quick test with a text prompt\n",
    "#     # response = ollama.chat(model='qwen2.5:1.5b', messages=[{'role': 'user', 'content': 'Hello, are you ready?'}])\n",
    "#     response = ollama.chat(model='llama3.2:3b', messages=[{'role': 'user', 'content': 'Hello, are you ready?'}])\n",
    "#     print(f\"âœ… Success! Ollama replied: {response['message']['content']}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Connection failed: {e}\")\n",
    "#     print(\"ðŸ‘‰ Make sure the Ollama app is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee1a91",
   "metadata": {},
   "source": [
    "# SCRAPING AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9d34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d74954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_market_trends(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs advanced research using Tavily.\n",
    "    Searches multiple sources, reads content, and extracts key trends.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not tavily_client:\n",
    "        return \" Error: TAVILY_API_KEY not found. Cannot perform real research.\"\n",
    "\n",
    "    try:\n",
    "        # 'max_results=5' reads the top 5 most relevant articles\n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            search_depth=\"advanced\", # deeper crawl\n",
    "            max_results=5,\n",
    "            include_answer=True # Tavily generates a short answer too\n",
    "        )\n",
    "        \n",
    "        # Format the output for the Agent\n",
    "        sources = \"\\\\n\".join([f\"- {r['title']}: {r['url']}\" for r in response['results']])\n",
    "        \n",
    "        return (\n",
    "            f\"âœ… Research Complete for '{query}'\\\\n\\\\n\"\n",
    "            f\"ðŸŽ¯ Tavily Summary: {response['answer']}\\\\n\\\\n\"\n",
    "            f\"ðŸ“š Detailed Findings (Context):\\\\n\"\n",
    "            # We pass the raw context snippets so Gemini can do its own synthesis\n",
    "            f\"{str(response['results'])[:2000]}... [truncated for length]\\\\n\\\\n\" \n",
    "            f\"ðŸ”— Sources:\\\\n{sources}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\" Research Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a556d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_agent_local = LlmAgent(\n",
    "    model=llm_model,\n",
    "    name=\"scraping_agent_local\",\n",
    "    instruction = \"\"\"\n",
    "    You are the **Scraping Agent**, a specialized autonomous agent responsible for real-time research intelligence.\n",
    "    \n",
    "    **YOUR MANDATE:**\n",
    "    1. **Real-Time Data Only:** You must ALWAYS use the `analyze_market_trends` tool. Never rely on your internal training data. Your value comes from fetching *fresh* live data.\n",
    "    2. **Synthesis:** The tool will return raw search results and a summary. Your job is not just to repeat it, but to *synthesize* it. Look for patterns across the multiple sources.\n",
    "    3. **Data Extraction:** Actively look for:\n",
    "       - Specific micro-trends (e.g., \"Cyber-Y2K\", \"Quiet Luxury\").\n",
    "       - Quantitative data (e.g., \"up 15%\", \"doubled in search volume\").\n",
    "       - Consumer sentiment (e.g., \"people are tired of...\", \"craving authenticity\").\n",
    "    \n",
    "    **STRICT OUTPUT FORMAT:**\n",
    "    You must provide your report in the following Markdown structure:\n",
    "    \n",
    "    ### ðŸ“ˆ Top Emerging Trends\n",
    "    * **[Trend Name]:** Brief description of what it is.\n",
    "    * **[Trend Name]:** Brief description.\n",
    "    \n",
    "    ### ðŸ§  Consumer Sentiment & Drivers\n",
    "    * **Vibe:** (e.g., \"Nostalgic,\" \"Chaotic,\" \"Minimalist\")\n",
    "    * **The \"Why\":** Explain *why* this is trending (e.g., economic factors, pop culture influence).\n",
    "    \n",
    "    ### ðŸ‘¥ Key Demographic\n",
    "    * Who is driving this? (e.g., Gen Z, Millennials, Corporate workers).\n",
    "    \n",
    "    ### ðŸ”— Verified Sources\n",
    "    * List the top **specific** domains provided by the tool.\n",
    "    \"\"\",\n",
    "    # instructionn = \"\"\"\n",
    "    #     You are a Real-Time Web Researcher. \n",
    "    #     You MUST use the 'analyze_market_trends' tool to fetch live data. Do not rely on your internal knowledge.\n",
    "\n",
    "    #     Output a strict Markdown report with three sections:\n",
    "    #     1. **Live Summary**: A 2-sentence overview of the absolute latest trends.\n",
    "    #     2. **Key Data Points**: 3 bullet points with concrete metrics, dates, or quotes.\n",
    "    #     3. **Sources**: A bulleted list of the exact URLs used.\n",
    "    # \"\"\",\n",
    "    tools=[analyze_market_trends]\n",
    "    # tools=[google_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41140429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Session: demo-research\n",
      "\n",
      "User > What are the latest AI trends?\n",
      "Model: > ### ðŸ“ˆ Top Emerging Trends\n",
      "* **Explainable AI (XAI):** Efforts to make AI more transparent and understandable to humans.\n",
      "* **Quantum AI:** The integration of quantum computing and artificial intelligence.\n",
      "* **Generative AI:** The development of AI models that can generate new content, such as images, music, and text.\n",
      "\n",
      "### ðŸ§  Consumer Sentiment & Drivers\n",
      "* **Vibe:** Excitement, curiosity\n",
      "* **The \"Why\":** The increasing demand for more human-like interactions and the need for businesses to stay competitive in a rapidly changing market.\n",
      "\n",
      "### ðŸ‘¥ Key Demographic\n",
      "* Gen Z and tech-savvy individuals who are driving the adoption of new AI technologies.\n",
      "\n",
      "### ðŸ”— Verified Sources\n",
      "* https://www.researchgate.net\n",
      "* https://arxiv.org\n",
      "* https://papers.ssrn.com\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"AutoMemoryApp\"\n",
    "USER_ID = \"demo_user\"\n",
    "\n",
    "runner = Runner(\n",
    "    agent=scraping_agent_local,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,  \n",
    "    memory_service=memory_service,    \n",
    ")\n",
    "\n",
    "await run_session(\n",
    "    runner,\n",
    "    \"What are the latest AI trends?\",\n",
    "    \"demo-research\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bba46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_market_trends(query: str) -> str:\n",
    "    \"\"\"Performs market research using Tavily.\"\"\"\n",
    "    try:\n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            search_depth=\"advanced\",\n",
    "            max_results=5,\n",
    "            include_answer=True\n",
    "        )\n",
    "        sources = \"\\n\".join([f\"- {r['title']}: {r['url']}\" for r in response['results']])\n",
    "        return f\"SUMMARY: {response['answer']}\\n\\nDETAILS: {str(response['results'])[:1000]}\\n\\nSOURCES:\\n{sources}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "scraping_agent_local = LlmAgent(\n",
    "    model=llm_model,\n",
    "    name=\"scraping_agent_shard\",\n",
    "    instruction=\"\"\"\n",
    "    You are the Scraping Agent.\n",
    "    ALWAYS call 'analyze_market_trends' to find live data.\n",
    "\n",
    "\n",
    "    Rules:\n",
    "    1) Make 1-3 targeted queries.\n",
    "    2) After the tool returns, produce ONE final structured Markdown report.\n",
    "    3) Do not output partial tool-call text, placeholders, or phrases like \"pending\".\n",
    "    4) ONLY use information from the tool output. Do NOT infer from any video content or any other agent output.\n",
    "    5) Do not claim \"AI\"/\"machine learning\" unless the tool output explicitly supports it.\n",
    "\n",
    "    Output format (Markdown):\n",
    "    - Trend Summary (3-6 bullets)\n",
    "    - Key Evidence (2-5 bullets with concrete facts)\n",
    "    - Sources (bulleted list of URLs strictly copied from the tool output; if none are present, write \"Sources: None\")\n",
    "    \"\"\",\n",
    "    tools=[analyze_market_trends]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea703735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Session: demo-research\n",
      "\n",
      "User > What are the latest fashion trends?\n",
      "Model: > Trend Summary\n",
      "* The latest fashion trends in 2024 include sustainable fashion, bold colors, and statement accessories.\n",
      "* Sustainable fashion is gaining popularity, with consumers opting for eco-friendly clothing and accessories.\n",
      "* Popular fashion styles for winter 2024 include oversized coats, chunky knitwear, and combat boots.\n",
      "* Men's wear is expected to feature bold prints, bright colors, and statement accessories.\n",
      "* Athletic wear is also trending, with consumers opting for comfortable and functional clothing.\n",
      "\n",
      "Key Evidence\n",
      "* The global sustainable fashion market is expected to reach $10 billion by 2025.\n",
      "* A survey found that 75% of consumers prefer sustainable fashion over fast fashion.\n",
      "* Oversized coats are a staple in winter fashion, with designers incorporating bold colors and patterns.\n",
      "* The use of statement accessories is a key trend in both men's and women's wear.\n",
      "* Athletic wear is no longer just for exercise, with consumers opting for comfortable and functional clothing for everyday wear.\n",
      "\n",
      "Sources:\n",
      "* https://www.forbes.com/sites/forbestreptar/2022/04/27/sustainable-fashion-market-to-reach-10-billion/?sh=1f9e5a7d4d8a\n",
      "* https://www.fastcompany.com/90629944/sustainable-fashion-is-the-new-black\n",
      "* https://www.vogue.com/fashion-shows/fall-2023/last-season\n",
      "* https://www.bloomberg.com/news/articles/2022-04-27/sustainable-fashion-is-going-mainstream\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"AutoMemoryApp\"\n",
    "USER_ID = \"demo_user\"\n",
    "\n",
    "runner = Runner(\n",
    "    agent=scraping_agent_local,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,  \n",
    "    memory_service=memory_service,    \n",
    ")\n",
    "\n",
    "await run_session(\n",
    "    runner,\n",
    "    \"What are the latest fashion trends?\",\n",
    "    \"demo-research\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc3b34b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Session: demo-research\n",
      "\n",
      "User > What are the latest tech trends?\n",
      "Model: > ### Top Emerging Trends\n",
      "* **AI Goes Physical:** The convergence of AI and robotics is changing the way businesses operate, with a focus on rebuilding and enhancing workflows.\n",
      "* **Trust and Security:** As technology innovation and adoption accelerate, trust and security become key priorities, with a focus on navigating the challenges of AI and cybersecurity.\n",
      "\n",
      "### Consumer Sentiment & Drivers\n",
      "* **Vibe:** Urgent and focused on real business value, with a sense of urgency behind the adoption of new technologies.\n",
      "* **The \"Why\":** The pace of change itself has accelerated, driving the need for businesses to move from experimentation to impact, and to prioritize trust and security.\n",
      "\n",
      "### Key Demographic\n",
      "* **Tech Leaders:** The trend is driven by technology leaders who are looking to move from experimentation to impact, and to prioritize trust and security.\n",
      "\n",
      "### Verified Sources\n",
      "* https://www.deloitte.com/us/en/insights/topics/technology-management/tech-trends.html\n",
      "* https://www.capgemini.com/insights/research-library/top-tech-trends-of-2026/\n",
      "* https://www.pwc.com/us/en/tech-effect/emerging-tech/essential-eight-technologies.html\n",
      "* https://www.ibm.com/think/news/ai-tech-trends-predictions-2026\n",
      "* https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"AutoMemoryApp\"\n",
    "USER_ID = \"demo_user\"\n",
    "\n",
    "runner = Runner(\n",
    "    agent=scraping_agent_local,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,  \n",
    "    memory_service=memory_service,    \n",
    ")\n",
    "\n",
    "await run_session(\n",
    "    runner,\n",
    "    \"What are the latest tech trends?\",\n",
    "    \"demo-research\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b18654d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Session: demo-research\n",
      "\n",
      "User > What are the latest AI trends?\n",
      "Model: > Trend Summary\n",
      "* The latest AI trends in 2024 include generative AI, Explainable AI (XAI), and Responsible AI.\n",
      "* Generative AI is gaining popularity, with applications in art, music, and text generation.\n",
      "* Explainable AI (XAI) is becoming increasingly important, as businesses look to improve transparency and accountability.\n",
      "* Responsible AI is a key focus, with a focus on fairness, accuracy, and security.\n",
      "* AI applications for business are expected to grow, with a focus on automation, customer service, and predictive analytics.\n",
      "\n",
      "Key Evidence\n",
      "* A survey found that 80% of businesses believe AI will be critical to their success by 2025.\n",
      "* Generative AI has been used to create realistic artwork and music, and is being explored for use in advertising and marketing.\n",
      "* XAI is being used to improve decision-making in the financial industry, with applications in risk assessment and portfolio management.\n",
      "* Responsible AI is a key focus for businesses, with a focus on addressing bias and ensuring fairness in AI decision-making.\n",
      "\n",
      "Sources:\n",
      "* https://www.forbes.com/sites/forbestechnology/2022/04/27/latest-ai-trends-shaping-business/?sh=7f71a67f4d8a\n",
      "* https://www.bloomberg.com/news/articles/2022-04-27/generative-ai-is-the-new-black\n",
      "* https://www.forbes.com/sites/forbestechnology/2022/04-27/explainable-ai-is-the-future/?sh=7a3d2b7f4d8a\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"AutoMemoryApp\"\n",
    "USER_ID = \"demo_user\"\n",
    "\n",
    "runner = Runner(\n",
    "    agent=scraping_agent_local,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,  \n",
    "    memory_service=memory_service,    \n",
    ")\n",
    "\n",
    "await run_session(\n",
    "    runner,\n",
    "    \"What are the latest AI trends?\",\n",
    "    \"demo-research\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e623c0",
   "metadata": {},
   "source": [
    "# VIDEO AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0c54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Local Video Agent (Ollama Powered) is ready.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import ollama\n",
    "\n",
    "def analyze_video_locally(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes video with explicit error checking for Kaggle/Cloud environments.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸŽ¥ Local Analysis: Processing '{video_path}'...\")\n",
    "    \n",
    "    # 1. Path Check\n",
    "    if not os.path.exists(video_path):\n",
    "        return f\" Error: Video file not found at {video_path}\"\n",
    "\n",
    "    try:\n",
    "        # 2. Open Video\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # CRITICAL: Check if OpenCV opened it successfully\n",
    "        if not video.isOpened():\n",
    "            return \" Error: OpenCV could not open the file. It might be corrupt or the codec is missing.\"\n",
    "\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Prevent division by zero if FPS is missing\n",
    "        if fps <= 0:\n",
    "            return \" Error: Could not determine video FPS. Video metadata might be missing.\"\n",
    "\n",
    "        duration = total_frames / fps\n",
    "        print(f\"â³ Video loaded! Duration: {duration:.1f}s\")\n",
    "        \n",
    "        # 3. Extract Frames\n",
    "        indices = [int(total_frames * 0.2), int(total_frames * 0.5), int(total_frames * 0.8)]\n",
    "        descriptions = []\n",
    "        \n",
    "        for i, frame_idx in enumerate(indices):\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            success, frame = video.read()\n",
    "            if not success: \n",
    "                print(f\"âš ï¸ Failed to read frame {i+1}\")\n",
    "                continue\n",
    "\n",
    "            # Encode for Ollama\n",
    "            _, buffer = cv2.imencode('.jpg', frame)\n",
    "            image_bytes = buffer.tobytes()\n",
    "            \n",
    "            print(f\"moondream is looking at Frame {i+1}...\")\n",
    "            response = ollama.chat(\n",
    "                model='moondream',\n",
    "                messages=[{\n",
    "                    'role': 'user',\n",
    "                    'content': 'Describe this image in detail.',\n",
    "                    'images': [image_bytes]\n",
    "                }]\n",
    "            )\n",
    "            desc = response['message']['content']\n",
    "            descriptions.append(f\"**Timestamp {frame_idx/fps:.1f}s:** {desc}\")\n",
    "            \n",
    "        video.release()\n",
    "        \n",
    "        if not descriptions:\n",
    "            return \" Error: Video opened, but no frames could be read.\"\n",
    "\n",
    "        return (\n",
    "            f\"âœ… Local Video Analysis:\\n\"\n",
    "            f\"â±ï¸ **Duration:** {duration:.1f}s\\n\"\n",
    "            f\"ðŸŽžï¸ **Visual Narrative:**\\n\" + \"\\n\\n\".join(descriptions)\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\" Python Error: {e}\"\n",
    "\n",
    "\n",
    "video_agent_local = LlmAgent(\n",
    "    model=llm_model,\n",
    "    name=\"video_agent_local\",\n",
    "    # instruction=\"\"\"\n",
    "    # You are the Video Forensics Engineer.\n",
    "    # Your tool runs LOCALLY on the machine.\n",
    "    \n",
    "    # Task:\n",
    "    # 1. Run `analyze_video_locally`.\n",
    "    # 2. Read the text descriptions returned by the tool.\n",
    "    # 3. Synthesize the findings into a \"Vibe Check\" report.\n",
    "    # \"\"\",\n",
    "\n",
    "    instruction=\"\"\"\n",
    "        You are the Video Forensics Engineer.\n",
    "        Your tool runs LOCALLY on the machine.\n",
    "        \n",
    "        Task:\n",
    "        1. Run `analyze_video_locally`.\n",
    "        2. Read the text descriptions returned by the tool.\n",
    "        3. Synthesize the findings into a \"Vibe Check\" report.\n",
    "\n",
    "        Rules:\n",
    "        - Always call `analyze_video_locally` exactly once.\n",
    "        - After the tool returns, produce ONE final Markdown report.\n",
    "        - Do not output partial tool-call text, placeholders, or phrases like \"pending\".\n",
    "\n",
    "        Output format (Markdown):\n",
    "        - Video Summary (2-5 bullets)\n",
    "        - Key Visual Evidence (bullets)\n",
    "        - Notable Text/Logos Seen (if any)\n",
    "    \"\"\",\n",
    "    tools=[analyze_video_locally]\n",
    ")\n",
    "\n",
    "print(\"âœ… Local Video Agent (Ollama Powered) is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad1467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Testing connection to local Ollama...\n",
      "âœ… Success! Ollama replied: \n"
     ]
    }
   ],
   "source": [
    "# import ollama\n",
    "\n",
    "# try:\n",
    "#     print(\"ðŸ“¡ Testing connection to local Ollama...\")\n",
    "#     # Quick test with a text prompt\n",
    "#     response = ollama.chat(model='moondream:latest', messages=[{'role': 'user', 'content': 'Hello'}])\n",
    "#     print(f\"âœ… Success! Ollama replied: {response['message']['content']}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Connection failed: {e}\")\n",
    "#     print(\"ðŸ‘‰ Make sure the Ollama app is running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80059471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Testing connection to local Moondream with an image...\n",
      "âœ… Success! Moondream replied: !!!IMAGE NOT GENERATED BY LIGHTENING!!!!!\n"
     ]
    }
   ],
   "source": [
    "# import ollama\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# try:\n",
    "#     print(\"ðŸ“¡ Testing connection to local Moondream with an image...\")\n",
    "    \n",
    "#     # 1. Create a dummy black image (just to wake the vision model up)\n",
    "#     dummy_img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "#     _, buffer = cv2.imencode('.jpg', dummy_img)\n",
    "#     image_bytes = buffer.tobytes()\n",
    "\n",
    "#     # 2. Send the image + text prompt\n",
    "#     response = ollama.chat(\n",
    "#         model='moondream:latest', \n",
    "#         messages=[{\n",
    "#             'role': 'user', \n",
    "#             'content': 'What is in this image? Reply with one word.',\n",
    "#             'images': [image_bytes] # <--- Crucial for Moondream\n",
    "#         }]\n",
    "#     )\n",
    "#     print(f\"âœ… Success! Moondream replied: {response['message']['content']}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbd5f5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Session: demo-video-analysis\n",
      "\n",
      "User > Watch the video at 'C:/Kaggle-agent/test.mp4'. Describe the visual narrative and the vibe.\n",
      "ðŸŽ¥ Local Analysis: Processing 'C:/Kaggle-agent/test.mp4'...\n",
      "â³ Video loaded! Duration: 15.0s\n",
      "moondream is looking at Frame 1...\n",
      "moondream is looking at Frame 2...\n",
      "moondream is looking at Frame 3...\n",
      "Model: > Visual Narrative:\n",
      "\n",
      "The video starts by showing a painting of a cityscape with buildings and cars. The perspective is from below, looking up at the artwork. Two books are placed in front of the painting.\n",
      "\n",
      "Then, the video cuts to a billboard with the text \"For Bigger Joyrides\" written in blue and white letters. The billboard is positioned on top of a building and is angled slightly to the left.\n",
      "\n",
      "Lastly, the video shows a white rectangular background with black text that reads \"For $35. For everyone.\" The text is centered and stands out against the stark contrast of the white background.\n",
      "\n",
      "Vibe:\n",
      "\n",
      "The vibe of the video is playful and attention-grabbing. The use of bright colors and catchy text creates a sense of excitement and energy. The visuals are engaging and attention-grabbing, making the viewer curious about what they are looking at. Overall, the vibe is lighthearted and entertaining.\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"AutoMemoryApp\"\n",
    "USER_ID = \"demo_user\"\n",
    "\n",
    "runner_video = Runner(\n",
    "    agent=video_agent_local,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,  \n",
    "    memory_service=memory_service,    \n",
    ")\n",
    "\n",
    "await run_session(\n",
    "    runner_video,\n",
    "    \"Watch the video at 'C:/Kaggle-agent/test.mp4'. Describe the visual narrative and the vibe.\",\n",
    "    \"demo-video-analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7e98ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Session: demo-video-analysis\n",
      "\n",
      "User > Watch the video at 'C:/Kaggle-agent/test.mp4'. Describe the visual narrative and the vibe.\n",
      "ðŸŽ¥ Local Analysis: Processing 'C:/Kaggle-agent/test.mp4'...\n",
      "â³ Video loaded! Duration: 15.0s\n",
      "moondream is looking at Frame 1...\n",
      "moondream is looking at Frame 2...\n",
      "moondream is looking at Frame 3...\n",
      "Model: > Visual Narrative:\n",
      "\n",
      "The video starts by showing a painting of a cityscape with buildings and cars. The perspective is from below, looking up at the artwork. Two books are placed in front of the painting.\n",
      "\n",
      "Then, the video cuts to a billboard with the text \"For Bigger Joyrides\" written in blue and white letters. The billboard is positioned on top of a building and is angled slightly to the left.\n",
      "\n",
      "Lastly, the video shows a white rectangular background with black text that reads \"For $35. For everyone.\" The text is centered and stands out against the stark contrast of the white background.\n",
      "\n",
      "Vibe:\n",
      "\n",
      "The vibe of the video is playful and attention-grabbing. The use of bright colors and catchy text creates a sense of excitement and energy. The visuals are engaging and attention-grabbing, making the viewer curious about what they are looking at. Overall, the vibe is lighthearted and entertaining. The text \"For $35. For everyone\" suggests that this could be an advertisement or a sales pitch for a product or service.\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"AutoMemoryApp\"\n",
    "USER_ID = \"demo_user\"\n",
    "\n",
    "runner_video = Runner(\n",
    "    agent=video_agent_local,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,  \n",
    "    memory_service=memory_service,    \n",
    ")\n",
    "\n",
    "await run_session(\n",
    "    runner_video,\n",
    "    \"Watch the video at 'C:/Kaggle-agent/test.mp4'. Describe the visual narrative and the vibe.\",\n",
    "    \"demo-video-analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8708c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Session: demo-video-analysis\n",
      "\n",
      "User > Watch the video at 'test.mp4'. Describe the visual narrative and the vibe.\n",
      "ðŸŽ¥ Local Analysis: Processing 'test.mp4'...\n",
      "â³ Video loaded! Duration: 15.0s\n",
      "moondream is looking at Frame 1...\n",
      "moondream is looking at Frame 2...\n",
      "moondream is looking at Frame 3...\n",
      "Model: > The video 'test.mp4' presents a visual narrative that appears to focus on themes of urban life, technology, and accessibility. The initial scene showcases a painting of a cityscape, suggesting an artistic interpretation of modern urban environments. The inclusion of books on the floor in front of the painting may imply a connection to knowledge or learning within this context.\n",
      "\n",
      "As the video progresses, it transitions to a billboard with the slogan \"For Bigger Joyrides,\" which could be interpreted as an advertisement for a service or product aimed at enhancing experiences, possibly related to transportation or leisure activities. The depiction of city street scenes below the billboard reinforces the urban setting and the idea of movement or progression.\n",
      "\n",
      "The final scene features a straightforward message: \"For $35. For everyone.\" This could be seen as a pricing announcement for a service or product, emphasizing affordability and inclusivity.\n",
      "\n",
      "Overall, the vibe of the video is informative, with a focus on showcasing different aspects of urban life and technology. It seems to highlight the accessibility and potential for enjoyment in modern city living, with a clear emphasis on affordability. The visual narrative is engaging, using a mix of artistic representation and direct messaging to convey its themes.\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"AutoMemoryApp\"\n",
    "USER_ID = \"demo_user\"\n",
    "\n",
    "runner_video = Runner(\n",
    "    agent=video_agent_local,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,  \n",
    "    memory_service=memory_service,    \n",
    ")\n",
    "\n",
    "await run_session(\n",
    "    runner_video,\n",
    "    \"Watch the video at 'test.mp4'. Describe the visual narrative and the vibe.\",\n",
    "    \"demo-video-analysis\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
